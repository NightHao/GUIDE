# Project Structure Overview

## ğŸ¯ Flow Chart

```mermaid
graph TD
    %% Input Data
    A[entities_result.json] --> B[entities_to_graph.py]
    ENV[OpenAI API Keys] --> C[process_entity_graph.py]

    %% Core Processing Pipeline
    B --> |entity_graph.json| C[process_entity_graph.py]
    C --> |processed_entity_graph.json| D[eg_node_description_optimizer.py]
    C --> |log.json| E[build_alias_list.py]
    E --> |alias_dict.json| F[agentic_flow_construction.py]
    D --> |optimized_entity_graph.json| F

    %% Prompt Engineering Pipeline
    G[complete_prompts/*.txt] --> H[merge_prompts.py]
    I[final_prompt.json] --> H
    H --> |merged_prompts/*.txt| J[prompt_eng.py]
    J --> |new_prompts/*.txt| K[run_final_prompt.py]

    %% Query Processing Pipeline
    L[retrieved_chunks_15.json] --> M[query_processor.py]
    F --> |Agentic Flow| M
    M --> |entities_chunks.json| N[Generated Answers]

    %% Evaluation Pipeline
    O[Ground Truth Data] --> P[agentic_flow_eval.py]
    N --> P
    K --> P
    P --> Q[Evaluation Results]

    %% Simple Processing
    R[final_prompt.txt] --> S[main.py]
    S --> T[Formatted Output]

    %% Testing
    U[Sample Files] --> V[test_single_file.py]

    %% Styling
    classDef inputFile fill:#e1f5fe
    classDef processFile fill:#f3e5f5
    classDef outputFile fill:#e8f5e8

    class A,ENV,G,I,L,O,R,U inputFile
    class B,C,D,E,F,H,J,K,M,P,S,V processFile
    class N,Q,T outputFile
```

## ğŸ“ Directory Structure

```
GUIDE/
â”œâ”€â”€ ğŸ“– README.md                          # Project overview and quick start
â”œâ”€â”€ ğŸ“‹ PROJECT_STRUCTURE.md               # This file - structure overview
â”œâ”€â”€
â”œâ”€â”€ ğŸ—‚ï¸ src/                              # Source Code (12 main files)
â”‚   â”œâ”€â”€ ğŸ—ï¸ entities_to_graph.py           # Step 1: Raw entities â†’ Graph
â”‚   â”œâ”€â”€ âš™ï¸ process_entity_graph.py        # Step 2: Graph processing & abbreviations
â”‚   â”œâ”€â”€ ğŸ¯ eg_node_description_optimizer.py # Step 3: Optimize descriptions
â”‚   â”œâ”€â”€ ğŸ”— build_alias_list.py           # Step 4: Build alias mappings
â”‚   â”œâ”€â”€ ğŸ¤– agentic_flow_construction.py  # Step 5: Multi-agent workflow
â”‚   â”œâ”€â”€ ğŸ¤ query_processor.py           # Step 6: Main query engine
â”‚   â”œâ”€â”€ ğŸ“ prompt_eng.py                # Prompt engineering & templates
â”‚   â”œâ”€â”€ ğŸ”„ merge_prompts.py             # Prompt merging utilities
â”‚   â”œâ”€â”€ â–¶ï¸ main.py                      # Simple prompt runner
â”‚   â”œâ”€â”€ ğŸš€ run_final_prompt.py          # Batch prompt processor
â”‚   â”œâ”€â”€ ğŸ§ª test_single_file.py          # Testing utilities
â”‚   â””â”€â”€ ğŸ“Š agentic_flow_eval.py         # Evaluation framework
â”œâ”€â”€
â”œâ”€â”€ ğŸ“¦ data/
â”‚   â”œâ”€â”€ ğŸ“¥ input/                        # Input data files
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ entities_result.json           # Raw entity extraction results
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ retrieved_chunks_15.json       # Question-chunk mappings
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ final_prompt.json             # Final prompt templates
â”‚   â”‚   â”œâ”€â”€ ğŸ“ complete_prompts/             # Source prompt files
â”‚   â”‚   â””â”€â”€ ğŸ“– README.md                     # Input data guide
â”‚   â”œâ”€â”€ âš¡ intermediate/                 # Processing intermediate files
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ entity_graph.json            # Initial entity graph
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ processed_entity_graph.json  # Processed graph
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ alias_dict.json              # Alias mappings
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ log.json                     # Processing logs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ entities_chunks.json         # Entity chunks
â”‚   â”‚   â”œâ”€â”€ ğŸ“ merged_prompts/              # Merged prompt files
â”‚   â”‚   â””â”€â”€ ğŸ“– README.md                     # Intermediate data guide
â”‚   â””â”€â”€ ğŸ“¤ output/                       # Final outputs
â”‚       â”œâ”€â”€ ğŸ“„ optimized_entity_graph.json  # Final optimized graph
â”‚       â”œâ”€â”€ ğŸ“„ o1_answers.json              # Generated answers
â”‚       â”œâ”€â”€ ğŸ“ new_prompts/                 # Processed prompts
â”‚       â”œâ”€â”€ ğŸ“ evaluation_results/          # Evaluation outputs
â”‚       â””â”€â”€ ğŸ“– README.md                     # Output data guide
â”œâ”€â”€
â”œâ”€â”€ ğŸ“š docs/                             # Documentation
â”‚   â”œâ”€â”€ ğŸ“‹ Research_CODE_Documentation.md   # Detailed file documentation
â”‚   â””â”€â”€ ğŸ“Š system_flowchart.md             # System flow diagrams
â”œâ”€â”€
â”œâ”€â”€ ğŸ§ª tests/                            # Test files and outputs
â”œâ”€â”€
â””â”€â”€ âš™ï¸ config/                           # Configuration
    â””â”€â”€ ğŸ“„ requirements.txt                 # Python dependencies
```

## ğŸš€ Execution Order

### Core Pipeline
1. **Graph Construction**: `entities_to_graph.py`
2. **Graph Processing**: `process_entity_graph.py`
3. **Description Optimization**: `eg_node_description_optimizer.py`
4. **Alias Building**: `build_alias_list.py`

### Query Processing
5. **Agentic Flow Setup**: `agentic_flow_construction.py`
6. **Query Processing**: `query_processor.py`

### Prompt Engineering (Optional)
- **Prompt Merging**: `merge_prompts.py`
- **Prompt Engineering**: `prompt_eng.py`
- **Batch Processing**: `run_final_prompt.py`

### Evaluation
- **System Evaluation**: `agentic_flow_eval.py`

## ğŸ“Š Data Flow Summary

```
Raw Data â†’ Graph Building â†’ Processing â†’ Optimization â†’ Query Engine
    â†“           â†“              â†“            â†“           â†“
Input/      Intermediate/  Intermediate/ Intermediate/ Output/
```

## ğŸ¯ Key Features

- âœ… **Modular Design**: Each component has a specific purpose
- âœ… **Clear Data Flow**: Inputs â†’ Intermediate â†’ Outputs
- âœ… **Comprehensive Documentation**: Every component documented
- âœ… **Software Engineering Best Practices**: Proper directory structure
- âœ… **Evaluation Framework**: Built-in performance measurement
- âœ… **Async Processing**: Efficient LLM API usage

## ğŸ”§ Quick Setup

1. Install dependencies: `pip install -r config/requirements.txt`
2. Set API key: `export OPENAI_API_KEY="your_key"`
3. Place input files in `data/input/`
4. Run pipeline scripts in order
5. Check results in `data/output/`